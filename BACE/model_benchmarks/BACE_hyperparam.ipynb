{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import MoleculeNet\n",
    "import torch\n",
    "import numpy as np\n",
    "import best_config\n",
    "from torch_geometric.data import DataLoader\n",
    "import optuna\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine:\n",
    "    def __init__(self, model, model_params, optimizer, device):\n",
    "        self.model = model\n",
    "        self.model_params = model_params\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "    def train(self, loader):\n",
    "        self.model.train()\n",
    "        # Enumerate over the data\n",
    "        final_loss = 0\n",
    "        for batch in loader:\n",
    "            # Use GPU\n",
    "            batch.to(self.device)  \n",
    "            # Reset gradients\n",
    "            self.optimizer.zero_grad() \n",
    "            # Passing the node features and the connection info\n",
    "            if self.model_params['has_edge_info']:\n",
    "                pred = self.model(batch.x.float(),\n",
    "                                batch.edge_attr.float(), \n",
    "                                batch.edge_index, \n",
    "                                batch.batch)\n",
    "            else:\n",
    "                pred = self.model(batch.x.float(), \n",
    "                                batch.edge_index, \n",
    "                                batch.batch)\n",
    "            # Calculating the loss and gradients\n",
    "            loss = self.loss_fn(pred, batch.y)\n",
    "            final_loss += loss.item()\n",
    "            loss.backward()\n",
    "            # Update using the gradients\n",
    "            self.optimizer.step()   \n",
    "        return final_loss / len(loader)\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "            self.model.eval()\n",
    "            final_loss = 0\n",
    "            for batch in data_loader:\n",
    "                batch.to(self.device)\n",
    "                if self.model_params['has_edge_info']:\n",
    "                    pred = self.model(batch.x.float(),\n",
    "                                    batch.edge_attr.float(), \n",
    "                                    batch.edge_index, \n",
    "                                    batch.batch)\n",
    "                else:\n",
    "                    pred = self.model(batch.x.float(), \n",
    "                                    batch.edge_index, \n",
    "                                    batch.batch)\n",
    "                loss = self.loss_fn(pred, batch.y)  \n",
    "                final_loss += loss.item()\n",
    "            return final_loss / len(data_loader)\n",
    "\n",
    "def run_training(params, save_model = False):\n",
    "    data = MoleculeNet(root=\".\", name=\"bace\")\n",
    "    data = data.shuffle()\n",
    "    #Specify device\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "    model_params = params\n",
    "    model_params['feature_size'] = data.num_features\n",
    "    model_params[\"edge_dim\"] = data[0].edge_attr.shape[1]\n",
    "    model_params[\"has_edge_info\"] = False\n",
    "    \n",
    "    model = eval('models.'+best_config.GLOBALPARAMETERS['model_this_run']+'(model_params)')\n",
    "    print(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Wrap data in a data loader\n",
    "    data_size = len(data)\n",
    "    NUM_GRAPHS_PER_BATCH = model_params['batch_size']\n",
    "    train_loader = DataLoader(data[:int(data_size * 0.8)], \n",
    "                        batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "    valid_loader = DataLoader(data[int(data_size * 0.8):int(data_size * 0.9)], \n",
    "                            batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "    test_loader = DataLoader(data[int(data_size * 0.9):], \n",
    "                            batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=model_params['learning_rate'])  \n",
    "\n",
    "    eng = Engine(model, model_params, optimizer, device)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = 50\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    losses = []\n",
    "    for epoch in range(100):\n",
    "        loss = eng.train(train_loader)\n",
    "        val_loss = eng.evaluate(valid_loader)\n",
    "        losses.append(loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | Train Loss {loss} | Valid Loss {val_loss}\")\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            if save_model:\n",
    "                torch.save(model.state_dict(), 'model.pt')\n",
    "        else:\n",
    "            early_stopping_counter +=1\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            break\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"model_layers\": trial.suggest_int(\"model_layers\", 1,7),\n",
    "        \"batch_size\": trial.suggest_categorical('batch_size', [64,32,128]),\n",
    "        \"learning_rate\": trial.suggest_loguniform('learning_rate', 1e-6,1e-3),\n",
    "        \"model_embedding_size\": trial.suggest_categorical(\"model_embedding_size\",[32, 64, 128, 256,512,1024]),\n",
    "        \"model_gnn_dropout_rate\": trial.suggest_uniform('model_gnn_dropout_rate', 0.01, 0.2),\n",
    "        \"model_linear_dropout_rate\": trial.suggest_uniform('model_linear_dropout_rate', 0.01, 0.2),\n",
    "        \"model_dense_neurons\": trial.suggest_categorical(\"model_dense_neurons\",[32, 64, 128, 256])\n",
    "    }\n",
    "    best_loss = run_training(params, save_model=False)\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-11 09:39:09,767]\u001b[0m A new study created in memory with name: no-name-67240057-7ba3-4740-8b19-db9335593e82\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): GCNConv(32, 32)\n",
      "    (1): GCNConv(32, 32)\n",
      "    (2): GCNConv(32, 32)\n",
      "    (3): GCNConv(32, 32)\n",
      "    (4): GCNConv(32, 32)\n",
      "    (5): GCNConv(32, 32)\n",
      "    (6): GCNConv(32, 32)\n",
      "  )\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (gnn_dropout): Dropout(p=0.103304528314003, inplace=False)\n",
      "  (linear_dropout): Dropout(p=0.038465025549285395, inplace=False)\n",
      "  (linear): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajeckidoyle/miniconda3/envs/my_torch_geometric/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0 | Train Loss 0.6991735160350799 | Valid Loss 0.6972493529319763\n",
      "Epoch 10 | Train Loss 0.6884873032569885 | Valid Loss 0.6793898344039917\n",
      "Epoch 20 | Train Loss 0.6891699075698853 | Valid Loss 0.6880953907966614\n",
      "Epoch 30 | Train Loss 0.687045156955719 | Valid Loss 0.6628091335296631\n",
      "Epoch 40 | Train Loss 0.6785351872444153 | Valid Loss 0.6737695038318634\n",
      "Epoch 50 | Train Loss 0.6712668120861054 | Valid Loss 0.6869352757930756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-11 09:39:31,546]\u001b[0m Trial 0 finished with value: 0.6324683427810669 and parameters: {'model_layers': 7, 'batch_size': 128, 'learning_rate': 0.0002341814653322955, 'model_embedding_size': 32, 'model_gnn_dropout_rate': 0.103304528314003, 'model_linear_dropout_rate': 0.038465025549285395, 'model_dense_neurons': 32}. Best is trial 0 with value: 0.6324683427810669.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_trial\n",
      "[0.6324683427810669]\n",
      "{'model_layers': 7, 'batch_size': 128, 'learning_rate': 0.0002341814653322955, 'model_embedding_size': 32, 'model_gnn_dropout_rate': 0.103304528314003, 'model_linear_dropout_rate': 0.038465025549285395, 'model_dense_neurons': 32}\n",
      "GCN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): GCNConv(32, 32)\n",
      "    (1): GCNConv(32, 32)\n",
      "    (2): GCNConv(32, 32)\n",
      "    (3): GCNConv(32, 32)\n",
      "    (4): GCNConv(32, 32)\n",
      "    (5): GCNConv(32, 32)\n",
      "    (6): GCNConv(32, 32)\n",
      "  )\n",
      "  (initial_conv): GCNConv(9, 32)\n",
      "  (gnn_dropout): Dropout(p=0.103304528314003, inplace=False)\n",
      "  (linear_dropout): Dropout(p=0.038465025549285395, inplace=False)\n",
      "  (linear): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Starting training...\n",
      "Epoch 0 | Train Loss 0.688896757364273 | Valid Loss 0.6915474832057953\n",
      "Epoch 10 | Train Loss 0.6876207590103149 | Valid Loss 0.6860975623130798\n",
      "Epoch 20 | Train Loss 0.6850154280662537 | Valid Loss 0.6983514726161957\n",
      "Epoch 30 | Train Loss 0.6837551832199097 | Valid Loss 0.6741690933704376\n",
      "Epoch 40 | Train Loss 0.6858801364898681 | Valid Loss 0.6669547259807587\n",
      "Epoch 50 | Train Loss 0.6785967469215393 | Valid Loss 0.7676718533039093\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction = 'minimize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "print('best_trial')\n",
    "trial_ = study.best_trial\n",
    "\n",
    "print(trial_.values)\n",
    "print(trial_.params)\n",
    "\n",
    "scores = run_training(trial_.params,save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "best_parameters = pd.DataFrame([trial_.params])\n",
    "best_parameters[\"Best Loss\"] = trial_.values\n",
    "best_parameters.to_csv('/home/rajeckidoyle/Documents/Classification/BACE_Classification/model_benchmarks/hyperparameter_files/best_parameters/best_parameters.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4e88f5f6f0c037f170d8b5f2f59d1377880a8e4b384253995bdaa7b6b72e05d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('my_torch_geometric')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
